<h1>   # Content Analysis, topic modelling, keyword extractor  </h1>


<pre>
import nltk
import pandas as pd
import gensim
from gensim import corpora
from gensim.models import LdaModel
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from collections import Counter

# Read data from CSV file
df = pd.read_csv('/content/drive/MyDrive/SMA_datasets_prac_exam/social_media_data.csv')

# nltk.download('stopwords')
# nltk.download('wordnet')
# nltk.download('punkt')

# Tokenization, stopword removal, and lemmatization
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    tokens = word_tokenize(text.lower())
    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]
    return filtered_tokens

df['tokens'] = df['text'].apply(preprocess_text)

# Create dictionary and document-term matrix
dictionary = corpora.Dictionary(df['tokens'])
corpus = [dictionary.doc2bow(tokens) for tokens in df['tokens']]

# Topic modeling using LDA
num_topics = 5  # Adjust the number of topics as needed
lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=20)

# Print topics
print("Topics:")
for topic_id in range(num_topics):
    print(f"Topic {topic_id + 1}: {lda_model.print_topic(topic_id)}")

# Keyword extraction
all_tokens = [token for tokens in df['tokens'] for token in tokens]
keyword_counter = Counter(all_tokens)

# Print most common keywords
num_keywords = 10  # Adjust the number of keywords as needed
print("\nTop Keywords:")
for keyword, count in keyword_counter.most_common(num_keywords):
    print(f"{keyword}: {count}")

# Display topics in a structured format
print("Identified Topics from LDA:")
for idx in range(num_topics):
    topic_info = lda_model.print_topic(idx, topn=10)  # Get the top 10 terms per topic
    topic_number = idx + 1
    words_and_weights = topic_info.split(" + ")
    print(f"\nTopic {topic_number}:")
    for item in words_and_weights:
        weight, word = item.split("*")
        print(f"  - {word.strip()} (Weight: {weight.strip()})")

# visualization
for idx, topic in enumerate(lda_model.show_topics(num_topics, formatted=False)):
    topic_terms = [term[0] for term in topic[1]]
    term_weights = [term[1] for term in topic[1]]

    plt.figure()
    plt.bar(topic_terms, term_weights, color='skyblue')
    plt.title(f"Topic {idx + 1}")
    plt.xlabel("Terms")
    plt.ylabel("Weights")
    plt.show()
</pre>



<h1>  # Location analysis   </h1>

<pre>
!pip install pandas matplotlib seaborn

import pandas as pd
import re # optional
import matplotlib.pyplot as plt
import seaborn as sns

# Load the tweet data from CSV
tweet_data = pd.read_csv('tweet_data.csv')

# List of keywords to approximate locations
location_keywords = [
    'New York', 'London', 'Paris', 'Tokyo', 'Los Angeles', 'Berlin',
    'Chicago', 'San Francisco', 'Dubai', 'Toronto', 'Boston', 'Portland', 'Denver'
]

# Function to find keywords in text
def find_locations(text, keywords):
    found_locations = []
    tweet_words = text.split()
    # Search for keywords in the text (case-insensitive)
    for keyword in keywords:
        # regular expression method:
        # if re.search(rf'\b{re.escape(keyword)}\b', text, re.IGNORECASE):
        #     found_locations.append(keyword)
        if keyword in tweet_words:
            found_locations.append(keyword)
    return found_locations


# Extract all locations from tweets using the keyword list
all_locations = []
for tweet in tweet_data['text']:  # Adjust column name as needed
    locations = find_locations(tweet, location_keywords)
    all_locations.extend(locations)

# Create a DataFrame from the extracted locations
locations_df = pd.DataFrame(all_locations, columns=['Location'])

# Count the frequency of each location using value_counts
location_counts = locations_df['Location'].value_counts()
print(location_counts)

# sort
location_counts = location_counts.sort_values(ascending=False)

# Get the top 10 most mentioned locations
top_locations = location_counts.head(10)
print(top_locations)

# Convert to a DataFrame for easier plotting
top_locations_df = top_locations.reset_index()
top_locations_df.columns = ['Location', 'Count']

# Plotting the top locations
plt.figure(figsize=(10, 6))
sns.barplot(x='Count', y='Location', data=top_locations_df, palette="viridis")
plt.xlabel('Number of Mentions')
plt.ylabel('Location')
plt.title('Top Locations Mentioned in Tweets')
plt.show()
</pre>
